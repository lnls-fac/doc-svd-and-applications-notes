\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{physics}
\usepackage{amsfonts, amsmath, amssymb}
\usepackage{indentfirst}
\usepackage{todonotes}
\title{Singular Value Decomposition and Applications}
\author{}
\date{August 2022}

\begin{document}

\maketitle

\section{Introduction}
\section{Matrices, vectors and products}
We begin by introducing some notation and interpretations of the multiplication operations among matrices with matrices and matrices with and vectors. These will be essential for interpreting Singular Value Decomposition intuitively. 

Throughout this text, vectors $\vb{a}\in\mathbb{C}^m$ are represented by $m\times 1$ column matrices $\mqty[a^1 & a^2 & \dots & a^m]^{\intercal}\in\mathbb{C}^{m \times 1} $. Vectors (and matrices) are bold-typed while its components (entries) are not. 

First we highlight two complementary ways to see matrices and their products. We can interpret a matrix $\vb{A}\in \mathbb{C}^{m\times n}$ as an array of $n$ column vectors $\vb{a}_i\in\mathbb{C}^{m\times 1}$,  with $i=1,\dots,n$:
\begin{equation}
   \vb{A}=\left[\begin{array}{cccc}
    \mid & \mid & & \mid \\
    \vb{a}_{1} & \vb{a}_{2} & \cdots & \vb{a}_{n} \\
    \mid & \mid & & \mid
    \end{array}\right],
\end{equation}
or as a column array of $m$ transpose vectors $\boldsymbol{\alpha}_i^\intercal=\mqty[\alpha_i^1, & \dots & \alpha_i^n]\in\mathbb{C}^{1\times n}$, $i=1,\dots, m$
\begin{equation}
    \vb{A} = \mqty[\boldsymbol{\alpha}_1^\intercal \\ \boldsymbol{\alpha}_2^\intercal \\ \vdots \\ \boldsymbol{\alpha}_m^\intercal].
\end{equation}
\todo[inline]{find out how to insert horizontal bars to represent the complete rows}
The transpose vectors $\boldsymbol{\alpha}_i^\intercal$ are the $i$th row of the matrix $\vb{A}$.
\todo[inline]{add a simple ecxample displaying the $\vb{a}$ and $\boldsymbol{\alpha}$ vectors}
The product of a matrix with a vector, say $\vb{Ax}$,  can be seen as the $m\times 1$ matrix whose $i$th row is the scalar product of the $\boldsymbol{\alpha_i}$ vector with $\vb{x}$ vector, \textit{i.e.} 
\begin{equation}
    \vb{Ax} = \mqty[\boldsymbol{\alpha}_1^\intercal \\ \boldsymbol{\alpha}_2^\intercal \\ \vdots \\ \boldsymbol{\alpha}_m^\intercal] \vb{ x} = \mqty[\boldsymbol{\alpha}_1^\intercal \vb{x}\\ \boldsymbol{\alpha}_2^\intercal \vb{x}\\ \vdots \\ \boldsymbol{\alpha}_m^\intercal\vb{x}].
\end{equation}
Alternatively, we can think of this product as a linear combination of the column vectors of $\vb{A}$, with the coefficients being the components $x^i$ of the vector $\vb{x}$:
\begin{equation}
   \vb{A}=\left[\begin{array}{cccc}
    \mid & \mid & & \mid \\
    \vb{a}_{1} & \vb{a}_{2} & \cdots & \vb{a}_{n} \\
    \mid & \mid & & \mid
    \end{array}\right]\mqty[x^1 \\ x^2 \\ \vdots \\ x^n] = \sum_i \vb{a}_i x^i
\end{equation}
\todo[inline]{add a simple example}
As for the products of matrices $\vb{A}\in\mathbb{C}^{m\times n}$ and $\vb{B}\in\mathbb{C}^{n\times k}$, we can see it as
\begin{equation}
    \begin{aligned}
        \vb{AB} & = \mqty[\boldsymbol{\alpha}_1^\intercal \\ \boldsymbol{\alpha}_2^\intercal \\ \vdots \\ \boldsymbol{\alpha}_m^\intercal] \mqty[\vb{b}_1 & \vb{b}_2 & \dots & \vb{b}_k] \\
        & = \mqty[\boldsymbol{\alpha}_1^\intercal\vb{b}_1 & \boldsymbol{\alpha}_1^\intercal\vb{b}_2 & \dots & \boldsymbol{\alpha}_1^\intercal\vb{b}_k\\
        \boldsymbol{\alpha}_2^\intercal\vb{b}_1 & \boldsymbol{\alpha}_2^\intercal\vb{b}_2 & \dots & \boldsymbol{\alpha}_2^\intercal\vb{b}_k\\
        \vdots & \vdots & \ddots & \vdots\\
        \boldsymbol{\alpha}_m^\intercal\vb{b}_m & \boldsymbol{\alpha}_1^\intercal\vb{b}_2 & \dots & \boldsymbol{\alpha}_m^\intercal\vb{b}_k]
    \end{aligned}\
\end{equation}
In this form it's clear why $\vb{A}^\intercal \vb{A}$, which equals
\begin{equation}
    \begin{aligned}
        \vb{A}^\intercal\vb{A} & = \mqty[\vb{a}_1^\intercal\vb{a}_1 & \vb{a}_1^\intercal\vb{a}_2 & \dots & \vb{a}_1^\intercal\vb{a}_k\\
        \vb{a}_2^\intercal\vb{a}_1 & \vb{a}_2^\intercal\vb{a}_2 & \dots & \vb{a}_2^\intercal\vb{a}_k\\
        \vdots & \vdots & \ddots & \vdots\\
        \vb{a}_m^\intercal\vb{a}_m & \vb{a}_1^\intercal\vb{a}_2 & \dots & \vb{a}_m^\intercal\vb{a}_k],
    \end{aligned}\
\end{equation}
is called the \textit{self-covariance matrix}. The diagonal entries are $\norm{\vb{a}_i}^2$, while the off-diagonal ones are the projections of the $i$th and $j$th column vectors of $\vb{A}$. In a matrix where the columns are highly correlated, $\vb{A}^\intercal\vb{A}$ is a dense matrix, whereas diagonal terms dominate if the columns are less correlated..  

We can also see the product of matrices $\vb{AB}$ in the list of columns $\times$ column rows format. 
\todo[inline]{add a simple example first}
\begin{equation}
    \vb{AB} = \mqty[\vb{a}_1 & \vb{a}_2 & \dots & \vb{a}_m ] \mqty[\boldsymbol{\beta}_1^\intercal\\ \boldsymbol{\beta}_2^\intercal\\ \vdots \\ \boldsymbol{\beta}_n^\intercal\\] = \vb{a}_1\vb{b}_1^\intercal + \vb{a}_2\vb{b}_2^\intercal+\dots+ \vb{a}_n\vb{b}_n^\intercal 
\end{equation}
add examples
\section{Unitary Matrices}
\end{document}
